import logging
from typing import List, Optional, Any, Union, Dict
from pathlib import Path
import shutil # For file copying/moving

import polars as pl

# Import global TEMP_DIR from core config
from app.core.config import TEMP_DIR
# Import Pydantic schemas for input validation
from app.core.schemas.input_schema import Input, Configure, ExternalDBSchema, LocalDBSchema
# Import service functions
from app.services.requests.fetch import fetch_data
from app.services.process.process import process
from app.services.export._zip import zip_export

logger = logging.getLogger(__name__)

def run_pipeline(
    task_id: str,
    inputs: List[Input],
    configuration: Configure, # The Configure object now holds db_config
) -> Dict[str, Union[str, int]]:
    """
    Orchestrates the data processing pipeline: fetches data from a database (external or local SQLite),
    processes it (renaming/reordering columns), and exports it to a specified format.
    All temporary and final output files for a task are stored in a dedicated task-specific directory.

    Args:
        task_id (str): A unique identifier for the current processing task.
        inputs (List[Input]): A list of Input objects specifying column transformations.
        configuration (Configure): Overall configuration for the task, including db_config and file_type.

    Returns:
        Dict[str, Union[str, int]]: A dictionary containing:
            - 'task_id': The ID of the completed task.
            - 'output_path': The absolute path to the generated output file.
            - 'file_type': The type of the generated output file.
            - 'file_size': The size of the generated output file in bytes.

    Raises:
        ValueError: For invalid input parameters or missing required configurations.
        RuntimeError: For failures during data fetching, processing, or export operations.
        FileNotFoundError: If an expected intermediate or final output file does not exist.
    """
    # Define a temporary task-specific directory within the global TEMP_DIR
    task_temp_dir = TEMP_DIR / task_id
    intermediate_csv_path: Optional[Path] = None # Path to the CSV generated by the 'process' service
    final_output_path: Optional[Path] = None    # Path to the final exported file (CSV, JSON, XLSX, or ZIP)

    try:
        # --- 1. Input Validation ---
        if not isinstance(task_id, str) or not task_id.strip():
            raise ValueError("Parameter 'task_id' must be a non-empty string.")
        if not isinstance(inputs, list) or not all(isinstance(i, Input) for i in inputs):
            raise ValueError("Parameter 'inputs' must be a list of Input objects.")
        if not isinstance(configuration, Configure):
            raise ValueError("Parameter 'configuration' must be an instance of Configure schema.")
        
        # Validate db_config existence within the configuration object
        if configuration.db_config is None:
            raise ValueError("Missing 'db_config' in the provided configuration. Cannot fetch data without it.")
        
        # Validate that db_config is one of the supported database schema types
        if not isinstance(configuration.db_config, (ExternalDBSchema, LocalDBSchema)):
             raise ValueError(f"Unsupported db_config type: {type(configuration.db_config).__name__}. "
                              "Expected ExternalDBSchema or LocalDBSchema.")


        logger.info(f"Pipeline: Starting for task_id='{task_id}'")
        
        # Ensure the task-specific temporary directory exists
        task_temp_dir.mkdir(parents=True, exist_ok=True)
        logger.debug(f"Pipeline: Task-specific output directory ensured at '{task_temp_dir.resolve()}'")

        # --- 2. Data Fetching ---
        logger.info(f"Pipeline: Fetching data using provided DB config for task_id='{task_id}'.")
        # The fetch_data function now accepts the combined Union type for db_config
        lazy_frame = fetch_data(configuration.db_config)
        
        if not isinstance(lazy_frame, pl.LazyFrame):
            # This check is important if fetch_data could return None or an unexpected type on error
            raise RuntimeError(f"Data fetching failed: Expected pl.LazyFrame, but received {type(lazy_frame).__name__}.")

        logger.debug(f"Pipeline: Data fetched as Polars LazyFrame for task_id='{task_id}'. Schema: {lazy_frame.schema}")

        # --- 3. Data Processing ---
        logger.info(f"Pipeline: Processing data for task_id='{task_id}' (applying transformations).")
        # The 'process' function will save its output CSV directly in the global TEMP_DIR
        intermediate_csv_path = process(lazy_frame, task_id, inputs)

        if not isinstance(intermediate_csv_path, Path):
            raise RuntimeError(f"Data processing failed: Expected Path from process(), but received {type(intermediate_csv_path).__name__}.")
        if not intermediate_csv_path.exists():
            raise FileNotFoundError(f"Data processing failed: Intermediate CSV file not found at '{intermediate_csv_path.resolve()}'.")

        logger.debug(f"Pipeline: Data processed successfully, intermediate CSV at: '{intermediate_csv_path.resolve()}'")

        # --- 4. Data Exporting to Final Format ---
        # Get the desired output file type from the configuration. Default to 'csv'.
        file_type = configuration.file_type
        if file_type is None: # Handle case where file_type might be explicitly None
            file_type = "csv"
        file_type = file_type.lower().strip() # Normalize to lowercase and remove whitespace

        logger.info(f"Pipeline: Preparing to export data as '{file_type}' for task_id='{task_id}'.")

        # The base filename for the final output will be the task_id
        base_filename_stem = task_id
        
        if file_type == "csv":
            # If the final output is CSV, copy the intermediate CSV to the task-specific directory
            final_output_path = task_temp_dir / f"{base_filename_stem}.csv"
            # Ensure the intermediate CSV is copied to its final, designated spot.
            shutil.copy(intermediate_csv_path, final_output_path)
            logger.debug(f"Pipeline: Exporting as CSV. Copied from '{intermediate_csv_path.name}' to '{final_output_path.resolve()}'.")
        elif file_type == "json":
            df = pl.read_csv(intermediate_csv_path) # Read intermediate CSV into DataFrame
            json_path = task_temp_dir / f"{base_filename_stem}.json"
            df.write_json(json_path) # Write DataFrame to JSON
            final_output_path = json_path
            logger.debug(f"Pipeline: Converted to JSON: '{final_output_path.resolve()}'")
        elif file_type == "xlsx":
            df = pl.read_csv(intermediate_csv_path) # Read intermediate CSV into DataFrame
            xlsx_path = task_temp_dir / f"{base_filename_stem}.xlsx"
            df.write_excel(xlsx_path) # Write DataFrame to XLSX (Polars supports single sheet directly)
            final_output_path = xlsx_path
            logger.debug(f"Pipeline: Converted to XLSX: '{final_output_path.resolve()}'")
        elif file_type == "zip":
            # The zip_export function takes the file to be zipped, task_id, and the target output directory
            final_output_path = zip_export(intermediate_csv_path, task_id, output_dir=task_temp_dir)
            logger.debug(f"Pipeline: Zipped CSV to: '{final_output_path.resolve()}'")
        else:
            raise ValueError(f"Unsupported export file type: '{file_type}'. Supported types: csv, json, xlsx, zip.")

        # Final checks on the output file
        if not final_output_path: # Ensure a path was actually set
            raise RuntimeError("Export operation failed: No final output file path was generated.")
        if not final_output_path.exists():
            raise FileNotFoundError(f"Export operation failed: Generated output file does not exist at '{final_output_path.resolve()}'.")

        file_size = final_output_path.stat().st_size # Get the size of the final output file
        logger.info(f"Pipeline: Task '{task_id}' completed successfully. Final output at: '{final_output_path.resolve()}' (Size: {file_size} bytes)")

        return {
            "task_id": task_id,
            "output_path": str(final_output_path.resolve()), # Return absolute path as string
            "file_type": file_type,
            "file_size": file_size
        }

    except ValueError as e:
        logger.error(f"Pipeline input validation failed for task_id='{task_id}': {e}")
        raise # Re-raise to be caught by API endpoint for HTTP response
    except FileNotFoundError as e:
        logger.error(f"Pipeline file error for task_id='{task_id}': {e}")
        raise
    except pl.ComputeError as e: # Catch Polars specific computation errors
        logger.error(f"Polars computation error during pipeline for task_id='{task_id}': {e}", exc_info=True)
        raise RuntimeError(f"Data processing or conversion error: {e}") from e
    except Exception as e:
        # Catch any other unexpected exceptions during pipeline execution
        error_msg = f"An unexpected error occurred during pipeline execution for task_id='{task_id}': {e}"
        logger.error(error_msg, exc_info=True)
        raise RuntimeError(error_msg) from e
    finally:
        # --- 5. Cleanup Intermediate Temporary Files ---
        # The 'process' function saves to TEMP_DIR / {task_id}.csv.
        # If this is not the final output, we should clean it up.
        if intermediate_csv_path and intermediate_csv_path.exists():
            # Only delete if the intermediate CSV is NOT the same as the final output path (e.g., for CSV export, they are identical after copy)
            if final_output_path is None or intermediate_csv_path != final_output_path:
                try:
                    intermediate_csv_path.unlink(missing_ok=True)
                    logger.debug(f"Cleaned up intermediate CSV: '{intermediate_csv_path.resolve()}'")
                except OSError as e:
                    logger.warning(f"Failed to delete intermediate CSV '{intermediate_csv_path.resolve()}': {e}")
        
        # The `task_temp_dir` itself (which holds the final output file) is NOT deleted here.
        # This allows the download endpoint to serve the file and provides a location for the final output.
        # If the `task_temp_dir` and its contents need to be deleted after download,
        # that logic should typically reside in the download endpoint's `call_on_close` callback
        # or be part of a separate cleanup service.
        logger.debug(f"Temporary directory '{task_temp_dir.resolve()}' for task '{task_id}' (containing final output) is preserved.")
